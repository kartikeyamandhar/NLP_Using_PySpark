{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"nlp\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(r\"C:\\Users\\Kartikeya Mandhar\\Desktop\\AWSMachineLearning\\bbc-text.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|     category|                text|\n",
      "+-------------+--------------------+\n",
      "|         tech|tv future in the ...|\n",
      "|     business|worldcom boss  le...|\n",
      "|        sport|tigers wary of fa...|\n",
      "|        sport|yeading face newc...|\n",
      "|entertainment|ocean s twelve ra...|\n",
      "|     politics|howard hits back ...|\n",
      "|     politics|blair prepares to...|\n",
      "|        sport|henman hopes ende...|\n",
      "|        sport|wilkinson fit to ...|\n",
      "|entertainment|last star wars  n...|\n",
      "|entertainment|berlin cheers for...|\n",
      "|     business|virgin blue share...|\n",
      "|     business|crude oil prices ...|\n",
      "|     politics|hague  given up  ...|\n",
      "|        sport|moya emotional af...|\n",
      "|     business|s korean credit c...|\n",
      "|     politics|howard backs stem...|\n",
      "|        sport|connors boost for...|\n",
      "|     business|japanese banking ...|\n",
      "|         tech|games maker fight...|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol = \"text\",outputCol = \"words\")\n",
    "regexTokenizer = RegexTokenizer(inputCol = \"text\",outputCol = \"words\",pattern = \"\\\\W\")\n",
    "\n",
    "countTokens = udf(lambda x:len(x),IntegerType())     #Word count for each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+\n",
      "|     category|                text|               words|\n",
      "+-------------+--------------------+--------------------+\n",
      "|         tech|tv future in the ...|[tv, future, in, ...|\n",
      "|     business|worldcom boss  le...|[worldcom, boss, ...|\n",
      "|        sport|tigers wary of fa...|[tigers, wary, of...|\n",
      "|        sport|yeading face newc...|[yeading, face, n...|\n",
      "|entertainment|ocean s twelve ra...|[ocean, s, twelve...|\n",
      "|     politics|howard hits back ...|[howard, hits, ba...|\n",
      "|     politics|blair prepares to...|[blair, prepares,...|\n",
      "|        sport|henman hopes ende...|[henman, hopes, e...|\n",
      "|        sport|wilkinson fit to ...|[wilkinson, fit, ...|\n",
      "|entertainment|last star wars  n...|[last, star, wars...|\n",
      "|entertainment|berlin cheers for...|[berlin, cheers, ...|\n",
      "|     business|virgin blue share...|[virgin, blue, sh...|\n",
      "|     business|crude oil prices ...|[crude, oil, pric...|\n",
      "|     politics|hague  given up  ...|[hague, , given, ...|\n",
      "|        sport|moya emotional af...|[moya, emotional,...|\n",
      "|     business|s korean credit c...|[s, korean, credi...|\n",
      "|     politics|howard backs stem...|[howard, backs, s...|\n",
      "|        sport|connors boost for...|[connors, boost, ...|\n",
      "|     business|japanese banking ...|[japanese, bankin...|\n",
      "|         tech|games maker fight...|[games, maker, fi...|\n",
      "+-------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|                text|               words|tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|tv future in the ...|[tv, future, in, ...|   806|\n",
      "|worldcom boss  le...|[worldcom, boss, ...|   332|\n",
      "|tigers wary of fa...|[tigers, wary, of...|   270|\n",
      "|yeading face newc...|[yeading, face, n...|   390|\n",
      "|ocean s twelve ra...|[ocean, s, twelve...|   287|\n",
      "|howard hits back ...|[howard, hits, ba...|   701|\n",
      "|blair prepares to...|[blair, prepares,...|   284|\n",
      "|henman hopes ende...|[henman, hopes, e...|   202|\n",
      "|wilkinson fit to ...|[wilkinson, fit, ...|   163|\n",
      "|last star wars  n...|[last, star, wars...|   253|\n",
      "|berlin cheers for...|[berlin, cheers, ...|   326|\n",
      "|virgin blue share...|[virgin, blue, sh...|   216|\n",
      "|crude oil prices ...|[crude, oil, pric...|   360|\n",
      "|hague  given up  ...|[hague, , given, ...|   324|\n",
      "|moya emotional af...|[moya, emotional,...|   498|\n",
      "|s korean credit c...|[s, korean, credi...|   311|\n",
      "|howard backs stem...|[howard, backs, s...|   439|\n",
      "|connors boost for...|[connors, boost, ...|   479|\n",
      "|japanese banking ...|[japanese, bankin...|   310|\n",
      "|games maker fight...|[games, maker, fi...|   497|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.select(\"text\",\"words\").withColumn(\"tokens\",countTokens(col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|                text|               words|tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|tv future in the ...|[tv, future, in, ...|   750|\n",
      "|worldcom boss  le...|[worldcom, boss, ...|   300|\n",
      "|tigers wary of fa...|[tigers, wary, of...|   248|\n",
      "|yeading face newc...|[yeading, face, n...|   349|\n",
      "|ocean s twelve ra...|[ocean, s, twelve...|   269|\n",
      "|howard hits back ...|[howard, hits, ba...|   625|\n",
      "|blair prepares to...|[blair, prepares,...|   269|\n",
      "|henman hopes ende...|[henman, hopes, e...|   198|\n",
      "|wilkinson fit to ...|[wilkinson, fit, ...|   165|\n",
      "|last star wars  n...|[last, star, wars...|   236|\n",
      "|berlin cheers for...|[berlin, cheers, ...|   311|\n",
      "|virgin blue share...|[virgin, blue, sh...|   202|\n",
      "|crude oil prices ...|[crude, oil, pric...|   342|\n",
      "|hague  given up  ...|[hague, given, up...|   291|\n",
      "|moya emotional af...|[moya, emotional,...|   466|\n",
      "|s korean credit c...|[s, korean, credi...|   294|\n",
      "|howard backs stem...|[howard, backs, s...|   412|\n",
      "|connors boost for...|[connors, boost, ...|   443|\n",
      "|japanese banking ...|[japanese, bankin...|   294|\n",
      "|games maker fight...|[games, maker, fi...|   471|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized = regexTokenizer.transform(df)\n",
    "regexTokenized.select(\"text\",\"words\").withColumn(\"tokens\",countTokens(col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex grabs each and every word, whereas normal lambda seperation looks for spaces giving rise to inaccurate tokens ( length count ) Therefore regex tokenization is always preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol = \"words\",outputCol = \"cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = regexTokenized.select(\"text\",\"words\").withColumn(\"tokens\",countTokens(col(\"words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = remover.transform(df_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = NGram(n=2,inputCol = \"words\",outputCol = \"bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_df = bigrams.transform(df_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             bigrams|\n",
      "+--------------------+\n",
      "|[tv future, futur...|\n",
      "|[worldcom boss, b...|\n",
      "|[tigers wary, war...|\n",
      "|[yeading face, fa...|\n",
      "|[ocean s, s twelv...|\n",
      "|[howard hits, hit...|\n",
      "|[blair prepares, ...|\n",
      "|[henman hopes, ho...|\n",
      "|[wilkinson fit, f...|\n",
      "|[last star, star ...|\n",
      "|[berlin cheers, c...|\n",
      "|[virgin blue, blu...|\n",
      "|[crude oil, oil p...|\n",
      "|[hague given, giv...|\n",
      "|[moya emotional, ...|\n",
      "|[s korean, korean...|\n",
      "|[howard backs, ba...|\n",
      "|[connors boost, b...|\n",
      "|[japanese banking...|\n",
      "|[games maker, mak...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigrams_df.select(\"bigrams\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF ( Term Frequency- Inverse Document Frequency )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=RegexTokenizer(inputCol = \"text\",outputCol = \"words\",pattern=\"\\\\W\")\n",
    "words_df = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+\n",
      "|     category|                text|               words|\n",
      "+-------------+--------------------+--------------------+\n",
      "|         tech|tv future in the ...|[tv, future, in, ...|\n",
      "|     business|worldcom boss  le...|[worldcom, boss, ...|\n",
      "|        sport|tigers wary of fa...|[tigers, wary, of...|\n",
      "|        sport|yeading face newc...|[yeading, face, n...|\n",
      "|entertainment|ocean s twelve ra...|[ocean, s, twelve...|\n",
      "|     politics|howard hits back ...|[howard, hits, ba...|\n",
      "|     politics|blair prepares to...|[blair, prepares,...|\n",
      "|        sport|henman hopes ende...|[henman, hopes, e...|\n",
      "|        sport|wilkinson fit to ...|[wilkinson, fit, ...|\n",
      "|entertainment|last star wars  n...|[last, star, wars...|\n",
      "|entertainment|berlin cheers for...|[berlin, cheers, ...|\n",
      "|     business|virgin blue share...|[virgin, blue, sh...|\n",
      "|     business|crude oil prices ...|[crude, oil, pric...|\n",
      "|     politics|hague  given up  ...|[hague, given, up...|\n",
      "|        sport|moya emotional af...|[moya, emotional,...|\n",
      "|     business|s korean credit c...|[s, korean, credi...|\n",
      "|     politics|howard backs stem...|[howard, backs, s...|\n",
      "|        sport|connors boost for...|[connors, boost, ...|\n",
      "|     business|japanese banking ...|[japanese, bankin...|\n",
      "|         tech|games maker fight...|[games, maker, fi...|\n",
      "+-------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol = \"words\",outputCol = \"rawFeatures\",numFeatures = 20)\n",
    "featurized = hashingTF.transform(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|     category|                text|               words|         rawFeatures|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|         tech|tv future in the ...|[tv, future, in, ...|(20,[0,1,2,3,4,5,...|\n",
      "|     business|worldcom boss  le...|[worldcom, boss, ...|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|tigers wary of fa...|[tigers, wary, of...|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|yeading face newc...|[yeading, face, n...|(20,[0,1,2,3,4,5,...|\n",
      "|entertainment|ocean s twelve ra...|[ocean, s, twelve...|(20,[0,1,2,3,4,5,...|\n",
      "|     politics|howard hits back ...|[howard, hits, ba...|(20,[0,1,2,3,4,5,...|\n",
      "|     politics|blair prepares to...|[blair, prepares,...|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|henman hopes ende...|[henman, hopes, e...|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|wilkinson fit to ...|[wilkinson, fit, ...|(20,[0,1,2,3,4,5,...|\n",
      "|entertainment|last star wars  n...|[last, star, wars...|(20,[0,1,2,3,4,5,...|\n",
      "|entertainment|berlin cheers for...|[berlin, cheers, ...|(20,[0,1,2,3,4,5,...|\n",
      "|     business|virgin blue share...|[virgin, blue, sh...|(20,[0,1,2,3,4,5,...|\n",
      "|     business|crude oil prices ...|[crude, oil, pric...|(20,[0,1,2,3,4,5,...|\n",
      "|     politics|hague  given up  ...|[hague, given, up...|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|moya emotional af...|[moya, emotional,...|(20,[0,1,2,3,4,5,...|\n",
      "|     business|s korean credit c...|[s, korean, credi...|(20,[0,1,2,3,4,5,...|\n",
      "|     politics|howard backs stem...|[howard, backs, s...|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|connors boost for...|[connors, boost, ...|(20,[0,1,2,3,4,5,...|\n",
      "|     business|japanese banking ...|[japanese, bankin...|(20,[0,1,2,3,4,5,...|\n",
      "|         tech|games maker fight...|[games, maker, fi...|(20,[0,1,2,3,4,5,...|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol = \"rawFeatures\",outputCol = \"features\")\n",
    "idf_model = idf.fit(featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|     category|            features|\n",
      "+-------------+--------------------+\n",
      "|         tech|(20,[0,1,2,3,4,5,...|\n",
      "|     business|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|(20,[0,1,2,3,4,5,...|\n",
      "|entertainment|(20,[0,1,2,3,4,5,...|\n",
      "|     politics|(20,[0,1,2,3,4,5,...|\n",
      "|     politics|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|(20,[0,1,2,3,4,5,...|\n",
      "|entertainment|(20,[0,1,2,3,4,5,...|\n",
      "|entertainment|(20,[0,1,2,3,4,5,...|\n",
      "|     business|(20,[0,1,2,3,4,5,...|\n",
      "|     business|(20,[0,1,2,3,4,5,...|\n",
      "|     politics|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|(20,[0,1,2,3,4,5,...|\n",
      "|     business|(20,[0,1,2,3,4,5,...|\n",
      "|     politics|(20,[0,1,2,3,4,5,...|\n",
      "|        sport|(20,[0,1,2,3,4,5,...|\n",
      "|     business|(20,[0,1,2,3,4,5,...|\n",
      "|         tech|(20,[0,1,2,3,4,5,...|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescale = idf_model.transform(featurized)\n",
    "rescale.select(\"category\",\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "model = cv.fit(words_df)\n",
    "result = model.transform(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(29457, {0: 43.0, 1: 17.0, 2: 26.0, 3: 24.0, 4: 8.0, 5: 13.0, 6: 6.0, 7: 8.0, 8: 12.0, 9: 7.0, 10: 6.0, 11: 3.0, 12: 3.0, 14: 1.0, 15: 6.0, 16: 10.0, 17: 2.0, 18: 3.0, 19: 2.0, 20: 6.0, 21: 1.0, 22: 6.0, 23: 4.0, 24: 9.0, 26: 1.0, 27: 3.0, 28: 12.0, 29: 1.0, 30: 2.0, 31: 2.0, 32: 3.0, 33: 2.0, 34: 4.0, 37: 2.0, 38: 3.0, 39: 4.0, 41: 2.0, 42: 1.0, 44: 3.0, 45: 1.0, 46: 7.0, 47: 3.0, 48: 7.0, 49: 4.0, 50: 1.0, 51: 5.0, 52: 2.0, 54: 6.0, 55: 1.0, 56: 1.0, 57: 1.0, 58: 1.0, 60: 1.0, 65: 2.0, 66: 2.0, 70: 2.0, 71: 1.0, 72: 3.0, 73: 7.0, 74: 1.0, 75: 3.0, 76: 3.0, 78: 2.0, 79: 1.0, 81: 5.0, 83: 1.0, 84: 1.0, 85: 1.0, 87: 4.0, 88: 1.0, 92: 2.0, 94: 1.0, 100: 2.0, 102: 2.0, 105: 1.0, 109: 3.0, 110: 1.0, 111: 2.0, 115: 2.0, 116: 1.0, 117: 2.0, 118: 1.0, 119: 1.0, 124: 1.0, 125: 2.0, 127: 1.0, 129: 1.0, 140: 3.0, 144: 1.0, 146: 1.0, 149: 1.0, 150: 4.0, 159: 3.0, 160: 2.0, 162: 4.0, 165: 7.0, 169: 1.0, 170: 1.0, 174: 1.0, 176: 1.0, 177: 13.0, 180: 2.0, 181: 1.0, 182: 1.0, 185: 3.0, 189: 1.0, 190: 1.0, 196: 1.0, 197: 1.0, 202: 1.0, 206: 2.0, 207: 1.0, 208: 1.0, 222: 3.0, 223: 1.0, 225: 1.0, 227: 1.0, 228: 1.0, 232: 1.0, 234: 1.0, 235: 2.0, 236: 2.0, 247: 1.0, 252: 1.0, 263: 2.0, 271: 2.0, 273: 1.0, 293: 1.0, 330: 1.0, 335: 1.0, 338: 1.0, 350: 1.0, 358: 1.0, 378: 1.0, 386: 1.0, 388: 1.0, 390: 1.0, 418: 1.0, 423: 1.0, 428: 2.0, 436: 1.0, 439: 1.0, 448: 2.0, 451: 1.0, 459: 1.0, 462: 1.0, 468: 2.0, 488: 1.0, 493: 1.0, 499: 1.0, 500: 1.0, 501: 2.0, 509: 4.0, 511: 2.0, 514: 4.0, 517: 2.0, 518: 1.0, 528: 1.0, 537: 1.0, 544: 1.0, 547: 1.0, 555: 1.0, 564: 1.0, 588: 2.0, 601: 3.0, 645: 1.0, 672: 2.0, 674: 2.0, 681: 1.0, 682: 1.0, 688: 1.0, 707: 2.0, 714: 1.0, 745: 3.0, 751: 1.0, 754: 1.0, 767: 1.0, 813: 2.0, 836: 2.0, 838: 1.0, 852: 1.0, 854: 1.0, 863: 1.0, 865: 1.0, 880: 1.0, 889: 1.0, 900: 1.0, 915: 1.0, 917: 2.0, 920: 1.0, 950: 1.0, 951: 1.0, 955: 1.0, 972: 1.0, 994: 3.0, 999: 2.0, 1012: 1.0, 1044: 4.0, 1048: 1.0, 1055: 1.0, 1079: 1.0, 1100: 1.0, 1131: 2.0, 1153: 3.0, 1172: 4.0, 1188: 1.0, 1189: 1.0, 1199: 3.0, 1219: 2.0, 1224: 1.0, 1229: 1.0, 1234: 1.0, 1261: 2.0, 1274: 1.0, 1300: 1.0, 1302: 1.0, 1329: 1.0, 1330: 3.0, 1357: 1.0, 1391: 1.0, 1399: 1.0, 1415: 1.0, 1418: 1.0, 1425: 1.0, 1428: 2.0, 1445: 1.0, 1475: 1.0, 1491: 1.0, 1493: 1.0, 1502: 2.0, 1510: 1.0, 1529: 1.0, 1560: 1.0, 1563: 1.0, 1596: 1.0, 1597: 1.0, 1606: 2.0, 1611: 1.0, 1656: 1.0, 1679: 2.0, 1701: 1.0, 1721: 1.0, 1745: 1.0, 1766: 1.0, 1769: 1.0, 1821: 1.0, 1861: 1.0, 1866: 1.0, 1877: 1.0, 1906: 1.0, 1922: 1.0, 1930: 3.0, 1934: 1.0, 1942: 1.0, 1964: 1.0, 2020: 1.0, 2080: 2.0, 2093: 1.0, 2140: 3.0, 2155: 1.0, 2168: 2.0, 2185: 1.0, 2214: 1.0, 2243: 3.0, 2420: 1.0, 2443: 1.0, 2481: 1.0, 2580: 1.0, 2604: 2.0, 2610: 1.0, 2670: 1.0, 2688: 2.0, 2705: 1.0, 2737: 1.0, 2757: 1.0, 2763: 1.0, 2798: 1.0, 2834: 1.0, 2918: 1.0, 2967: 1.0, 3060: 1.0, 3079: 1.0, 3154: 1.0, 3159: 1.0, 3232: 1.0, 3272: 1.0, 3285: 1.0, 3341: 1.0, 3347: 2.0, 3372: 1.0, 3475: 1.0, 3549: 1.0, 3643: 1.0, 3874: 1.0, 3964: 1.0, 3983: 2.0, 4128: 1.0, 4156: 1.0, 4204: 2.0, 4233: 1.0, 4276: 1.0, 4299: 1.0, 4346: 1.0, 4477: 2.0, 4592: 1.0, 4637: 1.0, 4749: 2.0, 4917: 1.0, 5172: 1.0, 5306: 1.0, 5349: 1.0, 5391: 2.0, 5525: 1.0, 5659: 2.0, 5722: 3.0, 5760: 1.0, 6074: 1.0, 6375: 1.0, 6656: 1.0, 6992: 1.0, 7108: 1.0, 7304: 1.0, 7345: 1.0, 7431: 2.0, 7508: 1.0, 7820: 1.0, 9368: 4.0, 9401: 1.0, 9403: 2.0, 9543: 1.0, 9757: 1.0, 9825: 1.0, 10041: 1.0, 12112: 1.0, 14155: 1.0, 14413: 3.0, 15867: 1.0, 16714: 1.0, 20969: 1.0, 24230: 1.0, 26589: 1.0, 27109: 1.0, 27125: 1.0, 27193: 1.0, 28340: 1.0, 28513: 1.0, 28614: 1.0, 28691: 1.0, 28948: 1.0}))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select(\"features\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
